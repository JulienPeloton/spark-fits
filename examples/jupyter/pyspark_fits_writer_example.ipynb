{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing data into FITS files with PySpark\n",
    "\n",
    "As of version 0.7 and earlier, spark-fits can only read and distribute FITS files but cannot write DataFrame data into FITS files on disk. Such a writer shall come one day, but in the meantime if you are using PySpark to perform your computation, writing into FITS files can be done in only very few steps.\n",
    "\n",
    "Note: this notebook presents _workarounds_ or _tricks_ to quickly write DataFrame data into FITS files. This is clearly not meant to be used in production mode!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial data set\n",
    "\n",
    "Let's load a test FITS file from the spark-fits repo. Note that at this stage, we could load data from any file formats supported by Spark as read and write processes are disconnected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"fits\").option(\"hdu\", 1).load(\"../../src/test/resources/test_file.fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+-----+-----+\n",
      "|    target|       RA|                 Dec|Index|RunId|\n",
      "+----------+---------+--------------------+-----+-----+\n",
      "|NGC0000000| 3.448297| -0.3387486324784641|    0|    1|\n",
      "|NGC0000001| 4.493667| -1.4414990980543227|    1|    1|\n",
      "|NGC0000002| 3.787274|  1.3298379564211742|    2|    1|\n",
      "|NGC0000003| 3.423602|-0.29457151504987844|    3|    1|\n",
      "|NGC0000004|2.6619017|  1.3957536426732444|    4|    1|\n",
      "+----------+---------+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's force the repartitionning of the data set in case there is only one partition.\n",
    "Also add one column with the partition ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions:  4\n",
      "+----------+----------+--------------------+-----+-----+------+\n",
      "|    target|        RA|                 Dec|Index|RunId|partId|\n",
      "+----------+----------+--------------------+-----+-----+------+\n",
      "|NGC0001880|  5.261077| -1.2750667383256107| 1880|    1|     0|\n",
      "|NGC0016891| 1.2029302|  0.8990373524969306|16891|    1|     0|\n",
      "|NGC0010745| 2.8954773|-0.20057668623844616|10745|    1|     0|\n",
      "|NGC0008653| 1.5082117|  0.9318336009561894| 8653|    1|     0|\n",
      "|NGC0006277| 5.1847734|0.023247432847754768| 6277|    1|     0|\n",
      "|NGC0006331|  2.391221| -0.2515119465585818| 6331|    1|     0|\n",
      "|NGC0000829| 0.9999453|  1.0383455102037864|  829|    1|     0|\n",
      "|NGC0012088| 1.6934042| -0.5576700853686516|12088|    1|     0|\n",
      "|NGC0004790| 4.1513066|  1.2978590130800844| 4790|    1|     0|\n",
      "|NGC0004439|  5.759273|  0.5185091392821048| 4439|    1|     0|\n",
      "|NGC0014863| 2.6574259| -1.3711896358852964|14863|    1|     0|\n",
      "|NGC0007658| 2.5824444| -1.1253206398660374| 7658|    1|     0|\n",
      "|NGC0007935|0.32474717| -0.3488153030427974| 7935|    1|     0|\n",
      "|NGC0010439| 0.7703432|   1.457895363476875|10439|    1|     0|\n",
      "|NGC0014544|  5.149415|  0.1899942085670414|14544|    1|     0|\n",
      "|NGC0003115| 1.1362475|  1.5456207050642399| 3115|    1|     0|\n",
      "|NGC0007127| 3.9032977|-0.22137753612918898| 7127|    1|     0|\n",
      "|NGC0017873| 3.6575491| -1.4139858814352761|17873|    1|     0|\n",
      "|NGC0016878| 1.9021842|   1.198457108772359|16878|    1|     0|\n",
      "|NGC0003258|  4.590992| 0.09290528526902597| 3258|    1|     0|\n",
      "+----------+----------+--------------------+-----+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "numPart = df.rdd.getNumPartitions()\n",
    "if numPart == 1:\n",
    "    df = df.repartition(4).withColumn(\"partId\", spark_partition_id())\n",
    "else:\n",
    "    df = df.withColumn(\"partId\", spark_partition_id())\n",
    "    \n",
    "print(\"Number of partitions: \", df.rdd.getNumPartitions())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing data with minimal effort\n",
    "\n",
    "In this first example, the user provides the names of columns, and data types.\n",
    "Since FITS has its own way to express data type, we need a data type conversion prior to writing the data.\n",
    "This is provided by the `toTFORM` routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "def toTFORM(value):\n",
    "    \"\"\" Simple data type converter.\n",
    "    \n",
    "    NOTE: Due to the nature of Python,\n",
    "    float will be converted to double, and int to long \n",
    "    automatically...\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    value: Any\n",
    "        Input value from which we want to know the \n",
    "        name of the type in the FITS language (TFORM).\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    out : str\n",
    "        Corresponding TFORM.\n",
    "\n",
    "    Examples\n",
    "    ----------\n",
    "    >>> toTFORM(1)\n",
    "    K\n",
    "    >>> toTFORM(\"toto\")\n",
    "    A4\n",
    "    >>> toTFORM(np.int16(10))\n",
    "    I\n",
    "    >>> toTFORM(3.4)\n",
    "    D\n",
    "    \"\"\"\n",
    "    if type(value) == str:\n",
    "        ft = \"A\" + str(len(value))\n",
    "    else:\n",
    "        tt = fits.column._dtype_to_recformat(type(value))[0]\n",
    "        ft = fits.column._convert_record2fits(tt)\n",
    "    return ft\n",
    "\n",
    "def write_from_user(part, colnames, coltypes, fitsname):\n",
    "    \"\"\" Write DataFrame data into FITS file on disk.\n",
    "    By default, there is one file per partition, \n",
    "    and data is written in the HDU 1.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    part : Iterator\n",
    "        Iterator containing data and partition ID.\n",
    "    colnames : List of str\n",
    "        List containing the names of the columns\n",
    "    coltypes : List of str\n",
    "        List containing the data types (FITS language - TFORM)\n",
    "    fitsname : str\n",
    "        Name for the output set of files (<blah>.fits). The set of files will\n",
    "        be then part<number>_<blah>.fits.\n",
    "    \"\"\"\n",
    "    # We assume the data contains \n",
    "    # data array (0, ..., N-1) & partition ID (Nth)\n",
    "    data = np.transpose([*part])\n",
    "    data_fits = data[0:-1]\n",
    "    partId = np.unique(data[-1])[0]\n",
    "    \n",
    "    # Create fake primary HDU\n",
    "    hdr = fits.Header()\n",
    "    primary_hdu = fits.PrimaryHDU(header=hdr)\n",
    "    \n",
    "    # HDU containing data\n",
    "    # Loop over columns\n",
    "    cols_ = []\n",
    "    for d, k, v in zip(data_fits, colnames, coltypes):\n",
    "        cols_.append(fits.Column(name=k, format=v, array=d))\n",
    "    cols = fits.ColDefs(cols_)\n",
    "    hdu1 = fits.BinTableHDU.from_columns(cols)\n",
    "\n",
    "    hdul = fits.HDUList([primary_hdu, hdu1])\n",
    "    \n",
    "    fnout = \"part{}_{}\".format(partId, fitsname)\n",
    "    hdul.writeto(fnout)\n",
    "    \n",
    "    # Return 0\n",
    "    yield 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Name for the output\n",
    "fitsname = \"from_write.fits\"\n",
    "\n",
    "# Names and dtypes\n",
    "colnames = df.columns\n",
    "oneRow = df.take(1)[0]\n",
    "coltypes = [toTFORM(i) for i in oneRow[:-1]]\n",
    "\n",
    "# Write data on disk. The count() is just here\n",
    "# to trigger the mapPartitions.\n",
    "df.rdd.mapPartitions(\n",
    "    lambda part: write_from_user(part, colnames, coltypes, fitsname)).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the process went right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:  20000  elements\n",
      "+----------+---------+--------------------+-----+-----+\n",
      "|    target|       RA|                 Dec|Index|RunId|\n",
      "+----------+---------+--------------------+-----+-----+\n",
      "|NGC0000000| 3.448297| -0.3387486324784641|    0|    1|\n",
      "|NGC0000001| 4.493667| -1.4414990980543227|    1|    1|\n",
      "|NGC0000002| 3.787274|  1.3298379564211742|    2|    1|\n",
      "|NGC0000003| 3.423602|-0.29457151504987844|    3|    1|\n",
      "|NGC0000004|2.6619017|  1.3957536426732444|    4|    1|\n",
      "+----------+---------+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- target: string (nullable = true)\n",
      " |-- RA: float (nullable = true)\n",
      " |-- Dec: double (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- RunId: integer (nullable = true)\n",
      "\n",
      "OUTPUT:  20000  elements\n",
      "+----------+------------------+--------------------+-----+-----+\n",
      "|    target|                RA|                 Dec|Index|RunId|\n",
      "+----------+------------------+--------------------+-----+-----+\n",
      "|NGC0000000|3.4482970237731934| -0.3387486324784641|    0|    1|\n",
      "|NGC0000001| 4.493667125701904| -1.4414990980543227|    1|    1|\n",
      "|NGC0000002|  3.78727388381958|  1.3298379564211742|    2|    1|\n",
      "|NGC0000003|3.4236021041870117|-0.29457151504987844|    3|    1|\n",
      "|NGC0000004|2.6619017124176025|  1.3957536426732444|    4|    1|\n",
      "+----------+------------------+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- target: string (nullable = true)\n",
      " |-- RA: double (nullable = true)\n",
      " |-- Dec: double (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- RunId: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.format(\"fits\").option(\"hdu\", 1).load(\"part*{}\".format(fitsname))\n",
    "print(\"INPUT: \", df.count(), \" elements\")\n",
    "df.drop(\"partId\").orderBy(\"target\").show(5)\n",
    "df.drop(\"partId\").printSchema()\n",
    "print(\"OUTPUT: \", df2.count(), \" elements\")\n",
    "df2.orderBy(\"target\").show(5)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "\n",
    "- When using `toTFORM`, float are automatically cast to double, and int to long... TBD. Of course you can enter formats manually to avoid this problem (not using `toTFORM`).\n",
    "- Data is written on local file system (not the DFS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing data from FITS HEADER\n",
    "\n",
    "If your structure has't change, you can also directly pass the input header when writing data.\n",
    "You could also define your own header corresponding to final data to be written, and pass it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "\n",
    "# Grab the input header\n",
    "data = fits.open(\"../../src/test/resources/test_file.fits\")\n",
    "header = data[1].header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def write_from_header(part, header, fitsname):\n",
    "    \"\"\" Write DataFrame data into FITS file on disk, using FITS header.\n",
    "    By default, there is one file per partition, \n",
    "    and data is written in the HDU 1.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    part : Iterator\n",
    "        Iterator containing data and partition ID.\n",
    "    header : astropy.io.fits.header.Header\n",
    "        Instance of the FITS header class.\n",
    "    fitsname : str\n",
    "        Name for the output set of files (<blah>.fits). The set of files will\n",
    "        be then part<number>_<blah>.fits.\n",
    "    \"\"\"\n",
    "    # We assume the data contains \n",
    "    # data array (0, ..., N-1) & partition ID (Nth)\n",
    "    data = np.transpose([*part])\n",
    "    data_fits = data[0:-1]\n",
    "    partId = np.unique(data[-1])[0]\n",
    "    \n",
    "    # Create fake primary header\n",
    "    hdr = fits.Header()\n",
    "    primary_hdu = fits.PrimaryHDU(header=hdr)\n",
    "    \n",
    "    # Grab column names and column data types\n",
    "    # from the header\n",
    "    names, types = [], []\n",
    "    for k, v in zip(header.keys(), header.values()):\n",
    "        if \"TTY\" in k:\n",
    "            names.append(v)\n",
    "        if \"TFO\" in k:\n",
    "            types.append(v)\n",
    "\n",
    "    # HDU containing data\n",
    "    # Loop over columns\n",
    "    cols_ = []\n",
    "    for d, k, v in zip(data_fits, names, types):\n",
    "        cols_.append(fits.Column(name=k, format=v, array=d))\n",
    "    cols = fits.ColDefs(cols_)\n",
    "    hdu1 = fits.BinTableHDU.from_columns(cols)\n",
    "\n",
    "    hdul = fits.HDUList([primary_hdu, hdu1])\n",
    "    \n",
    "    fnout = \"part{}_{}\".format(partId, fitsname)\n",
    "    hdul.writeto(fnout)\n",
    "    \n",
    "    yield 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Name for the output\n",
    "fitsname = \"from_header.fits\"\n",
    "\n",
    "# Write data on disk. The count() is just here\n",
    "# to trigger the mapPartitions.\n",
    "df.rdd.mapPartitions(\n",
    "    lambda part: write_from_header(part, header, fitsname)).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the process went right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:  20000  elements\n",
      "+----------+---------+--------------------+-----+-----+\n",
      "|    target|       RA|                 Dec|Index|RunId|\n",
      "+----------+---------+--------------------+-----+-----+\n",
      "|NGC0000000| 3.448297| -0.3387486324784641|    0|    1|\n",
      "|NGC0000001| 4.493667| -1.4414990980543227|    1|    1|\n",
      "|NGC0000002| 3.787274|  1.3298379564211742|    2|    1|\n",
      "|NGC0000003| 3.423602|-0.29457151504987844|    3|    1|\n",
      "|NGC0000004|2.6619017|  1.3957536426732444|    4|    1|\n",
      "+----------+---------+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- target: string (nullable = true)\n",
      " |-- RA: float (nullable = true)\n",
      " |-- Dec: double (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- RunId: integer (nullable = true)\n",
      "\n",
      "OUTPUT:  20000  elements\n",
      "+----------+---------+--------------------+-----+-----+\n",
      "|    target|       RA|                 Dec|Index|RunId|\n",
      "+----------+---------+--------------------+-----+-----+\n",
      "|NGC0000000| 3.448297| -0.3387486324784641|    0|    1|\n",
      "|NGC0000001| 4.493667| -1.4414990980543227|    1|    1|\n",
      "|NGC0000002| 3.787274|  1.3298379564211742|    2|    1|\n",
      "|NGC0000003| 3.423602|-0.29457151504987844|    3|    1|\n",
      "|NGC0000004|2.6619017|  1.3957536426732444|    4|    1|\n",
      "+----------+---------+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- target: string (nullable = true)\n",
      " |-- RA: float (nullable = true)\n",
      " |-- Dec: double (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- RunId: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.format(\"fits\").option(\"hdu\", 1).load(\"part*{}\".format(fitsname))\n",
    "print(\"INPUT: \", df.count(), \" elements\")\n",
    "df.drop(\"partId\").orderBy(\"target\").show(5)\n",
    "df.drop(\"partId\").printSchema()\n",
    "print(\"OUTPUT: \", df2.count(), \" elements\")\n",
    "df2.orderBy(\"target\").show(5)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "\n",
    "- Data is written on local file system (not the DFS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
